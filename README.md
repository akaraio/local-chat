# local-chat

The code creates a local Llama chat interface using Streamlit, which allows users to interact with installed OLLAMA models. It loads a list of available local models and provides a dropdown menu for selecting the model to use; it also retrieves the description of the selected model from a database. The code defines two main functions: conversation() handles the user's input and displays the conversation history, while responses() generates the assistant's responses in real-time.
